\documentclass{article}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[citestyle=authoryear]{biblatex}
\addbibresource{final.bib}
\usepackage{geometry}
\geometry{margin=1in}

\author{Tom Wallace}

\title{STAT 778 Final Project: Igeta, Takahashi, and Matsui 2018}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Overview}

This paper documents a simulation study based on \cite{igeta2018}. It
gives some background on their method; presents a software program coded in C; 
and conducts a simulation study using this program. 

I made little progress despite starting early and spending many hours on the project. 
I accept responsibility for this poor result.
That said, I believe \cite{igeta2018} to be a poorly-written and
inconsistently-notated paper with apparent errors.
One cannot write code without understanding the underlying math, 
and I spent so much time (unsuccessfully) trying to understand what
the authors were even talking about that I could not produce a working
program. I have left in the ``skeleton'' of sections that I was not able to
actually fulfill to make clear what my intent was.

\subsection{Background}

Overdispersion refers to a situation in which the variance of a dataset exceeds
that expected under the assumed statistical distribution. 
Overdispersion is common in count data. As a motivating example, consider the
Poisson distribution. A single parameter determines both the 
mean and variance: i.e., for $X \sim \mathrm{Poisson}(\lambda)$, $\mu =
\sigma^2 = \lambda$. As a consequence, if the mean and variance
differ in a dataset, the researcher cannot adjust parametric assumptions for 
one without also affecting the other. If not addressed, 
overdispersion results in distorted test statistics and estimated
standard errors. Clinical trials often feature count data: e.g., a trial of an
anti-epsilepsy treatment may use \textit{number of seizures over study period} as the outcome
variable. The serious consequences of statistical error in such settings demands
a rigorous method for dealing with overdispersion. \cite{igeta2018} is a new
entry to the large literature on this topic. In particular, it presents methods
for calculating statistical power and sample size in the presence of
misspecified variance.

\section{Methods}

\subsection{Overview}

The following is a brief summary of the methods proposed in \cite{igeta2018}.
The reader is encouraged to consult their paper for the necessary details. All
notation here follows that used in the original paper.

Consider a randomized control trial featuring $n$ subjects. 
Subjects are randomly assigned to a treatment or control group.
Let $n_i$ be the sample size of the $i$th group, with $i \in \{1, 2\}$.
Let $X_{ij}$ be an indicator variable of subject $j$'s group assignment. 
$X_{ij}=0$ indicates assignment to the control group.
Let $Y_ij$ be a count variable for patient $j$ in group $i$ over the follow-up period $[0, T_{ij}]$.
The expected value of $Y_{ij}$ is affected by a rate parameter $\lambda_i$.
The goal is to estimate the effect of treatment via a Poisson model 
$$
\lambda_i 
= \exp{(\beta_0 + \beta_1 X_{ij})}
$$
but $\lambda$ is overdispersed. 

\cite{igeta2018} propose a procedure for determining sample size and power in
the presence of overdispersion. The basic idea is that we assume there is some 
true variance function $V$ but this function is unknown. In practice we use
working variance function $\tilde{V}$. We don't actually know if we have
properly specified the true variance function and so would like sample size and power
calculations that are robust to misspecification.

\cite{igeta2018} employ a Wald-type test statistic using the sandwich-type
robust variance estimator under the null hypothesis:

\begin{equation}
	Z = \frac{\hat{\beta}_1}{\sqrt{n^{-1} \hat{W}_0}}
\end{equation}

They propose that the asymptotic power of the test using $Z$ 
with two-sided significance level $\alpha$ is\footnote{I think that 
this equation actually is wrong (as discussed in class) but was unable to derive the
correct version.}

\begin{equation}
	\mathrm{Pr} \left( Z > z_{1 - \alpha/ 2} \right) 
	=
	1 - \Phi 
	\left(
	z_{1 - \alpha / 2}
	\sqrt{\frac{W_0}{W_1}}
	-
	\sqrt{n}
	\frac{\beta_1}{\sqrt{W_1}}
	\right)
\end{equation}

 The sample size that provides power greater than or equal to $1 - \beta$ is

\begin{equation}
	n \geq \frac{(z_{1 - \alpha / 2} \sqrt{W_0} + z_{1 - \beta}
	\sqrt{W_1})^2}{\left( \log (\lambda_2 / \lambda_1 ) \right)^2}
\end{equation}

The reader should consult the paper to understand these equations, as
space constraints disallow a full explanation here. The chief claim
is that these methods are robust to misspecification of variance.

\subsection{Extensions}

To actually implement the methods of \cite{igeta2018}, 
the reader must significantly expand upon the terse descriptions
given by the authors. Here, I go into further detail on some key points.

\subsubsection{Calculating $\hat{\tilde{\phi}}_p^*$}

In order to calculate the $Z$-statistic given in (1), we must obtain
$\hat{\tilde{\phi}}^*$, which is a consistent estimate of the dispersion
parameter of $\tilde{V}$ under $H_0$. \cite{igeta2018} gives:

\begin{equation}
	\tilde{\phi}_p = \lim_{n \to \infty} \frac{\sum_{i=1}^2
	\sum_{j=1}^{n_i}(V_{ij} - \mu_{ij})\mu_{ij}^{p}}{\sum_{i=1}^{2}
	\sum_{j=1}^{n_i}\mu_{ij}^{2p}}
\end{equation}

and notes ``under the null hypothesis, the dispersion parameter is determined by
replacing $V_{ij}$ with $V_{ij}^*$ and $\mu_{ij}$ with $\mu_{ij}^*$.'' There are
many issues here that must be resolved.

One, the estimate given above is for the working variance function $\tilde{V} = \mu_{ij} + \tilde{\phi}_p
\mu_{ij}^p$. The authors say that ``we can employ other working variance
functions such as  $\phi \mu^p$ and $\phi_1 \mu \phi_2 \mu^2$.'' It is not clear
if this contrasting usage of $\tilde{\phi}$ vs. $\phi$ is intentional 
(elsewhere, a tilde indicates the ``working'' version of
something while the absence of a tilde indicates the ``true'' version of
something) or simply an error in notation. I proceed under the assumption that
it is an error.

Two, this equation is only valid for the working variance function $\tilde{V} = \mu_{ij} + \tilde{\phi}_p
\mu_{ij}^p$ and no estimator is offered for other working variance functions, even though
the authors use others in their simulation study (see Tables 2 and 3). In class, Dr.
Diao indicated that the following equation might work for $\phi \mu^p$:

\begin{equation}
	\tilde{\phi}_p =  \lim_{n \to \infty} \frac{\sum_{i=1}^2
	\sum_{j=1}^{n_i}V_{ij} \mu_{ij}^{p}}{\sum_{i=1}^{2}
	\sum_{j=1}^{n_i}\mu_{ij}^{2p}}
\end{equation}

Three, I believe that the use of $V$ is an error. $V$ indicates
the unknown true variance function. Because it is unknown, using it as part of
an estimator makes no sense. We instead should use $V^{(S)}$, some assumed form
of the true variance function. The authors discuss this $V$ vs. $V^{(S)}$
distinction but apparently err in discussing what equations the correction needs
to be applied to (using their equation numbering, they say (3) but really it is
(4)).

Taking all of this into account, I believe the following is the correct way to
calculate $\hat{\tilde{\phi}}_p^*$. Assume the true variance function
$V^{(S)}=\mu + \phi \mu^p$. Assume a working variance function of $\tilde{V} =
\tilde{\phi}\mu$. I use ``;'' to separate multiple superscripts (I have to use
these multiple superscripts to maintain correspondence to their original
notation). Then:

$$
	\hat{\tilde{\phi}}_p^* = \frac{\sum_{i=1}^2
	\sum_{j=1}^{n_i}V^{*; (S)}_{ij} \mu_{ij}^{*; p}}{\sum_{i=1}^{2}
	\sum_{j=1}^{n_i}\mu_{ij}^{*; 2p}}
$$

There is a further complication. It is given that $\mu_{ij}^* = T_{ij}
\exp{(\hat{\beta}_0^*)}$, where $\hat{\beta}_0^*$ is the maximum
quasi-likelihood estimate. So, we must find $\hat{\beta}_0^*$ before
proceeding. It is given that the maximum quasi-likelihood estimate of 
$\bm{\beta}=(\beta_0, \beta_1)$ is

$$
	\sum_{i=1}^2 \sum_{j=1}^{n_i} \bm{D}'_{ij} \tilde{V}_{ij}^{-1}(Y_{ij}-\mu_{ij})
	= \bm{0}
$$

One can see that this equation depends on $\tilde{V}_{ij}$, which takes
$\tilde{\phi}$ as a parameter, which is course what we are trying to estimate
via $\hat{\tilde{\phi}}$. This mutual dependence means we need to
simultaneously, iteratively estimate $\beta_0^*$ and $\tilde{\phi}$. This tactic 
apparently is common in generalized estimating equations (GEE) (\cite{zeger1988models}). 
I have no
experience in this area and was unable to implement the method.


\subsubsection{Calculating $\hat{\beta_1}$}

The maximum quasi-likelihood estimate of $\bm{\beta}=(\beta_0, \beta_1)$ is
given as:

\begin{equation}
	\sum_{i=1}^2 \sum_{j=1}^{n_i} \bm{D}'_{ij} \tilde{V}_{ij}^{-1}(Y_{ij}-\mu_{ij})
	= \bm{0}
\end{equation}

where 

$$
\bm{D}'_{ij} = \frac{\partial \mu_{ij}}{\partial \bm{\beta}}
$$ 

Although not directly stated, we will trust that these functions are convex (if
not, the resultant estimates are not unique). This system of equations can be numerically solved using the 
Newton-Raphson algorithm but first we must obtain a more tractable form. Since
$\mu_{ij} = T_{ij}\lambda_i = T_{ij}\exp{(\beta_0 + \beta_1 X_{ij})}$,

$$
\frac{\partial \mu_{ij}}{\partial \beta_0} = T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}
$$
$$
\frac{\partial \mu_{ij}}{\partial \beta_1} = T_{ij}X_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}
$$

For $\tilde{V}_{ij}$, we will only concern ourselves with the working variance
function of form $\tilde{V}_{ij} = \tilde{\phi} \mu_{ij} =
\tilde{\phi} T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}$ because this results in
answers not dependent on the working variance function, which per the previous
section we failed to estimate. Plugging everything back into
(4), we have

$$
\sum_{i=1}^2 \sum_{j=1}^{n_i}  T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}
\frac{1}{\tilde{\phi} T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}} 
(Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}) = 0
$$
$$
\sum_{i=1}^2 \sum_{j=1}^{n_i} T_{ij} X_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}
\frac{1}{\tilde{\phi} T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}} 
(Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}) = 0
$$

Which simplifies to
$$
\sum_{i=1}^2 \sum_{j=1}^{n_i} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}) = 0
$$
\begin{equation}
	\sum_{i=1}^2 \sum_{j=1}^{n_i} X_{ij} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}) = 0
\end{equation}

as the exponential expressions cancel out and $\tilde{\phi}$ is an arbitrary
constant that does not affect the solution. We now must take the derivatives of
the left-hand side of 
(5):

$$
\frac{\partial}{\partial \beta_0} 
\sum_{i=1}^2 \sum_{j=1}^{n_i} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})})
=
- \sum_{i=1}^2 \sum_{j=1}^{n_i} T_{ij}\exp{(\beta_0 + \beta_1 X_{ij})}
$$
$$
\frac{\partial}{\partial \beta_1} 
\sum_{i=1}^2 \sum_{j=1}^{n_i} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})})
=
- \sum_{i=1}^2 \sum_{j=1}^{n_i} T_{ij}X_{ij}\exp{(\beta_0 + \beta_1 X_{ij})}
$$
$$
\frac{\partial}{\partial \beta_0} 
	\sum_{i=1}^2 \sum_{j=1}^{n_i} X_{ij} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})})
=
	- \sum_{i=1}^2 \sum_{j=1}^{n_i} T_{ij} X_{ij} \exp{(\beta_0 + \beta_1 X_{ij})}
$$
$$
\frac{\partial}{\partial \beta_1} 
	\sum_{i=1}^2 \sum_{j=1}^{n_i} X_{ij} (Y_{ij} - T_{ij} \exp{(\beta_0 + \beta_1 X_{ij})})
=
	- \sum_{i=1}^2 \sum_{j=1}^{n_i} T_{ij} X_{ij}^2 \exp{(\beta_0 + \beta_1 X_{ij})}
$$

Thus, the Newton-Raphson algorithm for finding $\hat{\bm{\beta}}$ is (omitting the summation symbols for brevity):

\small
\begin{equation}
	\begin{bmatrix}
		\beta_{0}^{(n+1)} \\
		\beta_{1}^{(n+1)} \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		\beta_{0}^{(n)} \\
		\beta_{1}^{(n)} \\
	\end{bmatrix}
	- 
	\begin{bmatrix}
		- T_{ij}\exp{(\hat{\beta}_0^{(n)} + \hat{\beta}_1^{(n)}X_{ij})} & - T_{ij}X_{ij} \exp{(\hat{\beta}_0^{(n)}+ \hat{\beta}_1^{(n)}X_{ij})} \\
		- T_{ij} X_{ij} \exp{(\hat{\beta}_0^{(n)}+ \hat{\beta}_1^{(n)}X_{ij})} & - T_{ij}X_{ij}^2 \exp{(\hat{\beta}_0^{(n)}+ \hat{\beta}_1^{(n)}X_{ij})} \\
	\end{bmatrix}^{-1}
	\begin{bmatrix}
		Y_{ij} - T_{ij} \exp{(\hat{\beta}_0^{(n)}+ \hat{\beta}_1^{(n)}X_{ij})} \\
		X_{ij}(Y_{ij} - T_{ij} \exp{(\hat{\beta}_0^{(n)}+
		\hat{\beta}_1^{(n)}X_{ij}))} \\
	\end{bmatrix}
\end{equation}

\normalsize

I implemented this method in C but obtained a non-invertible matrix. I was
unable to ascertain whether this is a failure in my math or in my coding.

\subsubsection{Generating Overdispersed Data}

To replicate Table 2 in \cite{igeta2018}, we must generate over-dispersed data
according to some true variance function. We do so using the negative binomial
distribution. We need to identify the correct parameters $p$ and $r$ for the negative
binomial distribution that will result in the desired mean and variance. I
successfully did so with my program for a few values.

\section{Software Program}

\subsection{Overview}

We can empirically test these methods by the following procedure.

\begin{itemize}
	\item Randomly generate data that is overdispersed according to some
		true variance function; that has true treatment
		effect $\exp{(\beta_1)}$; and that has sample size equal to that
		recommended by (3) for the desired power $1-\beta$ calculated
		using (2).
	\item Conduct a test of $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$
		using (1). Record whether a type II error occurred.
	\item Conduct many iterations of the previous two steps.
		Calculate the proportion of type II errors:
		$$
		\bar{\beta} = \frac{1}{N}\sum_{t = 1}^N \beta_{(t)}
		$$

		where $\beta_{(t)}$ is an indicator of whether a type II error
		occured for trial $t$ and $N$ is the total number of iterations.
	\item Compare $1-\bar{\beta}$ to the asymptotic power calculated using (2). We
		hope that $1-\bar{\beta} \approx 1 - \beta$.
\end{itemize}

We conduct this procedure for multiple different true variance functions and
working variance functions and effect sizes. If the empirical power $1 - \bar{\beta}$ 
matches the asymptotic power across all specifications, we conclude that the claims of
\cite{igeta2018} are correct and that their methods are robust.

My software program failed to achieve the above.

\subsection{Technical Details}

[Deleted instructions on compilation, running, etc. because the program does
not work]

\section{Simulation Study}

[Was unable to carry out simulation study]

\section{Conclusion}

I have succeeded with all previous assignments in this class and so this
result was disappointing. The chief lesson I took from this assignment was the
importance of effective communication. The authors clearly intended for their
work to be practical and useful: the method applies to a problem commonly
encountered in industry, derivation of theorems is omitted from the main body of
the paper, and in general it seemed like the focus was on giving statisticians a
tool to solve a problem. But the poor quality of presentation means that the
target audience---working statisticians---probably will ignore the paper. It
takes too much time and effort to decipher the paper and fill in the gaps left
by the authors. In contrast, many of the most-cited statistics papers of all
time are very easy to read. \cite{bootstrap} and \cite{dempster1977maximum} are
good examples. This is not a coincidence: the clarity of these papers is part of
why their methods (bootstrap and EM, respectively) became so popular. One can
quickly read and grasp these methods and apply them to a problem; in contrast, \cite{igeta2018}
requires a big investment and working through the paper. I think
this is a valuable lesson to keep in mind. Effective communication of a method
matters just as much as its formal correctness.

\printbibliography[heading=bibnumbered]

\end{document}

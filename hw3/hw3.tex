\documentclass{article}

\usepackage{hyperref}

\usepackage{geometry}
\geometry{margin=1in}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\author{Tom Wallace}

\title{STAT 778 HW 3}

\begin{document}

\maketitle

\section*{Program Organization and Compilation}

Source code is contained in \texttt{hw3.c}. The program can be compiled with the
following command:

\begin{center}
	\texttt{gcc hw3.c -o hw3 -lm }
\end{center}

Executing the program requires one
argument, the name of the input file. An example command is:

\begin{center}
	\texttt{./hw3 HW2\_2018.dat}
\end{center}

\section*{Technical Notes}

This program finds maximum partial likelihood estimates (MPLE) for coefficients
$\bm{\beta}$ in the Cox proportional hazards model. This
section provides mathematical background on the computation of these estimates.

The log-likelihood function for the Cox proportional hazards model is:

\begin{equation}
\log L(\bm{\beta}) = \sum_{i=1}^n \delta_i \left( \bm{\beta'x}_i - \log \sum_{l
\in R(t_i)} \exp{(\bm{\beta' x}_l)}\right)
\end{equation}

with:
\begin{tabular}{l c l}
$t_i...t_n$ & = & unique observation times \\
$\delta_i$ & = & indicator function, which returns 0 if observation $t_i$ is
right-censored \\
$\bm{x}_i$ & = & vector of covariates associated with individual observed at
$t_i$ \\
$\bm{\beta}$ & = & vector of coefficients associated with covariates \\
$R(t_i)$ & = & risk set at $t_i$
\end{tabular}

\bigskip

The first order partial derivatives are:

\begin{equation}
\frac{\partial \log L}{\partial \beta_j} = 
	\sum_{i=1}^n 
	\delta_i
	\left( 
	x_{ij} - \frac{\sum_{l \in R(t_i)} x_{lj} \exp(\bm{\beta'x}_l)}
	{ \sum_{l \in R(t_i)} \exp(\bm{\beta'x}_l)}
	\right)
\end{equation}

It can be shown that these functions are concave, and so the their roots---i.e.,
the values of $\bm{\beta}$ for which the above functions equal $\bm{0}$---are the MPLE
estimators $\hat{\bm{\beta}}$. The Newton-Raphson algorithm is used to numerically
estimate the roots (there is no closed-form solution). This algorithm also
requires the derivatives of the function to be solved. Thus, we also need the
second-order partial derivatives of the log likelihood function.

\begin{equation}
\frac{\partial^2 \log L}{\partial \beta_j \partial \beta_k} = 
-
\sum_{i=1}^n \
\delta_i
	\left( 
	\frac{\sum_{l \in R(t_i)} x_{lj}x_{lk} \exp(\bm{\beta'x}_l)}
	{\sum_{l \in R(t_i)} \exp(\bm{\beta'x}_l)}
- 
\frac{
	\left(\sum_{l \in R(t_i)} x_{lj} \exp(\bm{\beta'x}_l)\right)
	\left(\sum_{l \in R(t_i)} x_{lk} \exp(\bm{\beta'x}_l)\right)
}
	{
		\left(
		\sum_{l \in R(t_i)} \exp(\bm{\beta'x}_l)
		\right)^2
		}
	\right)
\end{equation}

Once we have the Jacobian $\bm{J}$ and Hessian $\bm{H}$, the multivariate
Newton-Raphson algorithm is:

\begin{equation}
	\bm{\beta}_{n+1} = \bm{\beta}_n -
	\bm{H}^{-1}(\bm{\beta}_n)\bm{J}(\bm{\beta}_n)
\end{equation}

In our case of two covariates:
\begin{equation}
	\begin{bmatrix}
		\beta_{1, n+1} \\
		\beta_{2, n+1} \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		\beta_{1, n} \\
		\beta_{2, n} \\
	\end{bmatrix}
	-
	\begin{bmatrix}
		\frac{\partial^2 \log L}{\partial \beta_1^2}\left( \beta_{1,
		n}, \beta_{2, n} \right) &

		\frac{\partial^2 \log L}{\partial \beta_1 \partial \beta_2}\left( \beta_{1,
		n}, \beta_{2, n} \right) 
		
		\\

		\frac{\partial^2 \log L}{\partial \beta_2 \beta_1}\left( \beta_{1,
		n}, \beta_{2, n} \right) &

		\frac{\partial^2 \log L}{\partial \beta_2^2}\left( \beta_{1,
		n}, \beta_{2, n} \right) 
	\end{bmatrix}^{-1}
	\begin{bmatrix}
		\frac{\partial \log L}{\partial \beta_1}\left( \beta_{1, n},
		\beta_{2, n}\right) \\
		\frac{\partial \log L}{\partial \beta_2} \left( \beta_{1, n},
		\beta_{2, n} \right)\\
	\end{bmatrix}
\end{equation}

Note that $\bm{H}$ is square and symmetric. If it is positive definite (and it
is), the Cholesky decomposition can be used to efficiently obtain the inverse.

The algorithm iterates until an arbitrary quality of approximation is obtained:
\begin{equation}
	\bm{\beta}_{n} - \bm{\beta}_{n-1} \leq \epsilon
\end{equation}

\section*{Verification and Validation}
This program's results for $\bm{\hat{\beta}}$ were compared to those obtained using
the \texttt{coxph} function from the \texttt{survival} package in R. As depicted
in Table 1, the two are essentially identical.

\begin{table}[h!]
	\caption{Verification and validation}
	\centering
	\begin{tabular}{|l r r r r|}
		\hline
		& & & &\\
		& $\hat{\beta}_1$ & $SE(\hat{\beta_1})$ & $\hat{\beta}_2$ & $SE(\hat{\beta_2})$ \\
		& & & &\\
		\hline
		& & & &\\
		\textbf{R \texttt{survival} package}  &           & & &           \\
		& & & &\\
		\textbf{This C program}               &           & & &       \\
		& & & &\\
		\hline
	\end{tabular}
\end{table}

\end{document}
